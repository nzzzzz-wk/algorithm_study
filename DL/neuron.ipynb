{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播 单神经元 DL3\n",
    "### 描述\n",
    "实现一个单神经元的前向传播函数，使用sigmoid激活函数进行二分类预测。这是深度学习中最基本的神经网络单元。\n",
    "$$mse=∑(predictions_i- lables_i)^2$$\n",
    " \n",
    "### 输入描述：\n",
    "函数接收4个参数：\n",
    "1. features：二维列表，每行是一个样本的特征向量\n",
    "2. labels：一维列表，包含对应的二分类标签（0或1）\n",
    "3. weights：一维列表，权重向量\n",
    "4. bias：浮点数，偏置值\n",
    "### 输出描述：\n",
    "返回一个元组，包含两个元素：\n",
    "1. 预测概率列表：每个样本通过sigmoid函数后的预测概率（保留4位小数）\n",
    "2. MSE值：预测概率与真实标签之间的均方误差（保留4位小数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def single_neuron_model(features, labels, weights, bias):\n",
    "    x = np.dot(features,weights)+bias\n",
    "    probabilities = 1/(1+np.exp(-x))\n",
    "    mse = np.mean((probabilities - np.array(labels))**2)\n",
    "    return [round(p,4) for p in probabilities],round(mse,4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = np.array([[1, 2], [2, 3]])\n",
    "    labels = np.array([0,1])\n",
    "    weights = np.array([0.5,0.5])\n",
    "    bias = 0.0\n",
    "    print(single_neuron_model(features, labels, weights, bias))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具有反向传播的单神经元 DL11 \n",
    "### 描述\n",
    "实现一个单神经元的训练函数，使用sigmoid激活函数和均方误差(MSE)损失，通过反向传播算法更新权重和偏置。这是深度学习中最基本的神经网络单元实现。\n",
    " \n",
    "\n",
    "### 输入描述：\n",
    "函数接收6个参数：\n",
    "1. features：二维列表，每行是一个样本的特征向量\n",
    "2. labels：一维列表，包含对应的二分类标签（0或1）\n",
    "3. initial_weights：一维列表，初始权重\n",
    "4. initial_bias：浮点数，初始偏置值\n",
    "5. learning_rate：浮点数，学习率\n",
    "6. epochs：整数，训练轮数\n",
    "### 输出描述：\n",
    "返回一个元组，包含三个元素：\n",
    "1. 更新后的权重列表（保留4位小数）\n",
    "2. 更新后的偏置值（保留4位小数）\n",
    "3. 每个epoch的MSE值列表（保留4位小数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivate(x):\n",
    "    return x * (1-x)\n",
    "\n",
    "def forward(x,y,w,b):\n",
    "    out = np.dot(x,w)+b\n",
    "    pred = sigmoid(out)\n",
    "    mse = np.mean((pred-labels)**2)\n",
    "    return pred,mse\n",
    "\n",
    "def backward(x,y,dy,w,b,lr):\n",
    "    dz = dy * sigmoid_derivate(y)\n",
    "    dw = np.dot(x.T,dz)*2/ len(x) # 2 mse 没有 /2，所以这里加2 平均梯度\n",
    "    db = np.sum(dz)*2/ len(x)\n",
    "\n",
    "    w -= dw * lr\n",
    "    b -= db * lr\n",
    "    return w,b\n",
    "\n",
    "def train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs):\n",
    "    w = initial_weights\n",
    "    b = initial_bias\n",
    "    mses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        pred,loss = forward(features,labels,w,b)\n",
    "        mses.append(round(loss,4))\n",
    "        dy =  pred - labels\n",
    "        w,b = backward(features,pred,dy,w,b,learning_rate)\n",
    "    return [round(ww,4) for ww in w],round(b,4),mses\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features = np.array([[1, 2], [2, 3]])\n",
    "    labels = np.array([0,1])\n",
    "    initial_weights = np.array([0.5,0.5])\n",
    "    initial_bias = 0.0\n",
    "    learning_rate = 0.1\n",
    "    epochs = 2\n",
    "    print(train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/weixin_54856108/article/details/140878689\n",
    "反向传播算法的主要步骤如下：\n",
    "\n",
    "计算输出层误差：根据误差函数计算预测输出与真实值之间的误差。\n",
    "计算隐藏层误差：利用链式法则将误差反向传递到各隐藏层。\n",
    "更新权重和偏置：根据计算出的梯度调整每个神经元的权重和偏置。\n",
    "\n",
    "\n",
    "- dw 和 db 的计算方式都来源于梯度下降的 均值梯度 计算，而 db 可以使用 np.sum(dz)，但通常使用 np.mean(dz) 使其梯度更稳定。如果不除以 len(self.X)，梯度更新可能会 随着样本数量增加而变大，导致训练不稳定。np.sum 梯度下降更新的步长会更大（相当于 lr * len(self.X),lr需要缩小 以补偿db过大\n",
    "- 乘 2 是因为 MSE 的导数带有 2(y - t)\n",
    "- 如果使用 交叉熵损失（如二元交叉熵 BCE），它的导数不会有 2，所以 dw 也不需要乘 2。\n",
    "\n",
    "- 如果 MSE 公式里有 1/2，梯度计算时不用乘 2。\n",
    "- 如果 MSE 公式里没有 1/2，梯度计算时需要乘 2。\n",
    "- 两种方式最终效果一样，只是第一种（带 1/2）让梯度公式更简洁，不需要额外乘 2，所以常见于数学推导。\n",
    "- 代码实现时，可能为了简化代码而不加 1/2，这样计算梯度时就必须额外乘 2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.1970, w = [ 0.55513493 -0.74698749 -0.13639967], b = 3.6322\n",
      "Epoch 100: Loss = 0.1705, w = [ 0.85969309 -0.73424896 -0.41548077], b = 3.3403\n",
      "Epoch 200: Loss = 0.1470, w = [ 1.17626542 -0.69422327 -0.6520017 ], b = 3.0638\n",
      "Epoch 300: Loss = 0.1326, w = [ 1.42651293 -0.65878009 -0.83136286], b = 2.8490\n",
      "Epoch 400: Loss = 0.1255, w = [ 1.60201554 -0.63337484 -0.95605497], b = 2.6989\n",
      "Epoch 500: Loss = 0.1222, w = [ 1.7211743  -0.61608743 -1.04063892], b = 2.5970\n",
      "Epoch 600: Loss = 0.1207, w = [ 1.80276829 -0.60428467 -1.09862738], b = 2.5272\n",
      "Epoch 700: Loss = 0.1200, w = [ 1.85977831 -0.59607013 -1.13920832], b = 2.4784\n",
      "Epoch 800: Loss = 0.1196, w = [ 1.90042816 -0.59023394 -1.16818578], b = 2.4436\n",
      "Epoch 900: Loss = 0.1194, w = [ 1.9299146  -0.58601313 -1.18923062], b = 2.4184\n",
      "Final outputs: [0.39453113 0.51582874 0.63528961 0.43427633]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid 激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Sigmoid 导数（用于反向传播）\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\"\n",
    "        input_size: 输入特征的维度\n",
    "        \"\"\"\n",
    "        # 初始化权重和偏置\n",
    "        self.w = np.random.randn(input_size)  # 权重: 向量 (input_size,)\n",
    "        self.b = np.random.randn()  # 偏置: 标量\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"前向传播: 计算神经元输出\"\"\"\n",
    "        self.X = np.array(X)  # 输入样本 (样本数, 特征数)\n",
    "        self.z = np.dot(self.X, self.w) + self.b  # 计算线性组合 (样本数,)\n",
    "        self.y = sigmoid(self.z)  # 计算激活值 (样本数,)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dy, lr=0.1):\n",
    "        \"\"\"反向传播: 计算梯度并更新参数\"\"\"\n",
    "        dz = dy * sigmoid_derivative(self.y)  # 计算对 z 的梯度 (样本数,)\n",
    "\n",
    "        # 计算梯度\n",
    "        dw = np.dot(self.X.T, dz) / len(self.X)  # (特征数,) - 均值梯度\n",
    "        db = np.mean(dz)  # (标量) - 偏置梯度\n",
    "\n",
    "        # 更新权重和偏置\n",
    "        self.w -= lr * dw\n",
    "        self.b -= lr * db\n",
    "\n",
    "    def train(self, X, target, epochs=1000, lr=0.1):\n",
    "        \"\"\"训练神经元 (支持多个样本)\"\"\"\n",
    "        X = np.array(X)  # (样本数, 特征数)\n",
    "        target = np.array(target)  # (样本数,)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)  # 前向传播\n",
    "            loss = np.mean(0.5 * (y_pred - target) ** 2)  # 均方误差 (MSE)\n",
    "            dy = y_pred - target  # 计算损失对输出的梯度\n",
    "            self.backward(dy, lr)  # 反向传播\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss:.4f}, w = {self.w}, b = {self.b:.4f}\")\n",
    "\n",
    "# 3 维输入，4 个样本\n",
    "X = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [2, 3, 4]\n",
    "]  # (4, 3) 样本矩阵\n",
    "\n",
    "target = [1, 0, 1, 0]  # (4,) 目标输出\n",
    "\n",
    "# 训练神经元\n",
    "input_size = 3\n",
    "neuron = Neuron(input_size)\n",
    "neuron.train(X, target, epochs=1000, lr=0.1)\n",
    "\n",
    "# 训练后测试\n",
    "print(\"Final outputs:\", neuron.forward(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
